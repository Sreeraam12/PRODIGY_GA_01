# -*- coding: utf-8 -*-
"""genaitask-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gKd3pnxKF400C5w-mzjHYI6iJUt2A00k

**Install required packages**
"""

!pip install transformers datasets accelerate

"""**Upload your dataset bold text**"""

from google.colab import files
uploaded = files.upload()
file_path = list(uploaded.keys())[0]

"""**Read lines from the uploaded file**"""

with open(file_path, 'r', encoding='utf-8') as f:
    lines = f.readlines()

"""**Create HuggingFace dataset**"""

from datasets import Dataset

data = {"text": lines}
dataset = Dataset.from_dict(data)

"""**Load GPT-2 tokenizer**"""

from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # Add padding token

"""**Tokenize the dataset**"""

def tokenize_function(example):
    tokenized = tokenizer(example["text"], truncation=True, padding="max_length", max_length=512)
    tokenized["labels"] = tokenized["input_ids"].copy()  # Add labels for training
    return tokenized

tokenized_datasets = dataset.map(tokenize_function, batched=True)

"""**Load GPT-2 model**"""

from transformers import GPT2LMHeadModel
model = GPT2LMHeadModel.from_pretrained("gpt2")

"""**Training arguments**"""

from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    per_device_train_batch_size=4,
    num_train_epochs=8,
    save_steps=500,
    save_total_limit=1,
    logging_steps=100,
    fp16=True,                # Enable if using GPU
    report_to="none"          # Disable W&B or other logging
)

"""**Disable W&B entirely**"""

import os
os.environ["WANDB_DISABLED"] = "true"

"""**Train the model**"""

from transformers import Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
)

trainer.train()

"""**Generate text using the fine-tuned model**"""

from transformers import pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

prompt = "Tell me about Diwali in simple terms."
output = generator(prompt, max_length=100, num_return_sequences=1)
print(output[0]["generated_text"])